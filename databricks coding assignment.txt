# Task 1: Vehicle Maintenance Data Ingestion
from pyspark.sql import SparkSession
spark = SparkSession.builder \
.appName("DLT Exercise") \
.getOrCreate()
file_path = "dbfs:/FileStore/vehicle_maintenance.csv"
try:
# Attempt to read the CSV
attendance_df = spark.read.csv(file_path, header=True, inferSchema=True)
# Save as Delta table
attendance_df.write.format("delta").mode("overwrite").save("/delta/vehicle_maintenance_raw")
except Exception as e:
print(f"Error encountered: {str(e)}")

# Task 2: Data Cleaning
# Load raw data from Delta table
maintenance_raw_df = spark.read.format("delta").load("/delta/vehicle_maintenance_raw")

# Filter valid rows (ServiceCost and Mileage must be positive)
cleaned_df = maintenance_raw_df.filter((maintenance_raw_df.ServiceCost > 0) &
(maintenance_raw_df.Mileage > 0))

# Remove duplicate records based on VehicleID and Date
cleaned_df = cleaned_df.dropDuplicates(["VehicleID", "Date"])

# Save cleaned data to a new Delta table
cleaned_df.write.format("delta").mode("overwrite").save("/delta/cleaned_vehicle_maintenance")

# Task 3: Vehicle Maintenance Analysis
from pyspark.sql.functions import sum

# Load cleaned data from Delta table
cleaned_df = spark.read.format("delta").load("/delta/cleaned_vehicle_maintenance")

# Calculate the total maintenance cost for each vehicle
total_cost_df =
cleaned_df.groupBy("VehicleID").agg(sum("ServiceCost").alias("TotalMaintenanceCost"))

# Identify vehicles with mileage > 30,000
high_mileage_df = cleaned_df.filter(cleaned_df.Mileage > 30000)

# Save analysis results to Delta tables
total_cost_df.write.format("delta").mode("overwrite").save("/delta/total_maintenance_cost")
high_mileage_df.write.format("delta").mode("overwrite").save("/delta/high_mileage_vehicles")

# Task 5: Data Governance with Delta Lake
# Use VACUUM to remove old data versions
spark.sql("VACUUM `/delta/cleaned_vehicle_maintenance` RETAIN 7 HOURS")

# Use DESCRIBE HISTORY to check table history
spark.sql("DESCRIBE HISTORY `/delta/cleaned_vehicle_maintenance`").show(truncate=False)

# Task 1: Movie Ratings Data Ingestion
from pyspark.sql import SparkSession
spark = SparkSession.builder \
.appName("DLT Exercise") \
.getOrCreate()
file_path = "dbfs:/FileStore/movie_ratings.csv"
try:
# Attempt to read the CSV
ratings_df = spark.read.csv(file_path, header=True, inferSchema=True)
# Save as Delta table

attendance_df.write.format("delta").mode("overwrite").save("/delta/movie_ratings_raw")
except Exception as e:
print(f"Error encountered: {str(e)}")

# Task 2: Data Cleaning
# Load raw data from Delta table
ratings_raw_df = spark.read.format("delta").load("/delta/movie_ratings_raw")

# Filter valid ratings (between 1 and 5)
cleaned_df = ratings_raw_df.filter((ratings_raw_df.Rating >= 1) & (ratings_raw_df.Rating <= 5))

# Remove duplicates (same UserID and MovieID)
cleaned_df = cleaned_df.dropDuplicates(["UserID", "MovieID"])

# Save cleaned data to a new Delta table
cleaned_df.write.format("delta").mode("overwrite").save("/delta/cleaned_movie_ratings")

# Task 3: Movie Rating Analysis
from pyspark.sql.functions import avg

# Load cleaned data from Delta table
cleaned_df = spark.read.format("delta").load("/delta/cleaned_movie_ratings")

# Calculate the average rating for each movie
avg_rating_df = cleaned_df.groupBy("MovieID").agg(avg("Rating").alias("AvgRating"))

# Identify the movies with the highest and lowest average ratings
highest_rated_df = avg_rating_df.orderBy("AvgRating", ascending=False).limit(1)
lowest_rated_df = avg_rating_df.orderBy("AvgRating", ascending=True).limit(1)

# Save analysis results to Delta table

avg_rating_df.write.format("delta").mode("overwrite").save("/mnt/delta/movie_ratings_analysis")

# Task 4: Time Travel and Delta Lake History
# Update some ratings
updated_df = cleaned_df.withColumn("Rating", when(cleaned_df.MovieID == "M001",
5).otherwise(cleaned_df.Rating))

# Save the updated data to Delta table
updated_df.write.format("delta").mode("overwrite").save("/mnt/delta/cleaned_movie_ratings")

# Rollback to a previous version using time travel (e.g., version 0)
previous_version_df = spark.read.format("delta").option("versionAsOf",
0).load("/mnt/delta/cleaned_movie_ratings")

# View history of changes
spark.sql("DESCRIBE HISTORY '/delta/cleaned_movie_ratings'").show(truncate=False)

# Task 5: Optimize Delta Table
# Perform Z-ordering on the MovieID column
spark.sql("OPTIMIZE '/delta/cleaned_movie_ratings' ZORDER BY (MovieID)")

# Use VACUUM to clean up old data versions
spark.sql("VACUUM '/delta/cleaned_movie_ratings' RETAIN 0 HOURS")

# Task 1: Data Ingestion - Reading Data from Various Formats
# Ingest CSV Data (Student Information):
student_csv_path = "/FileStore/student_info.csv"

student_df = spark.read.csv(student_csv_path, header=True, inferSchema=True)
student_df.show()

# Ingest JSON Data (City Information):
city_df = spark.read.json("/Filestore/city_info.json")
city_df.show()

# Ingest Parquet Data (Hospital Information):
hospital_df = spark.read.parquet("/path/to/hospital_info.parquet")
hospital_df.show()

# Ingest Delta Table (Hospital Records):
try:
hospital_delta_df = spark.read.format("delta").load("/delta/hospital_records")
hospital_delta_df.show()
except Exception as e:
print(f"Error reading Delta table: {str(e)}")

# Task 2: Writing Data to Various Formats
# Write Student Data to CSV:
student_df.write.csv("/path/to/output/student_data.csv", header=True, mode="overwrite")

# Write City Data to JSON:
city_df.write.json("/Filestore/output/city_data.json", mode="overwrite")

# Write Hospital Data to Parquet:
hospital_df.write.parquet("/Filestore/output/hospital_data.parquet", mode="overwrite")

# Write Hospital Data to Delta Table:
hospital_df.write.format("delta").mode("overwrite").save("/delta/hospital_data")

# Task 3: Running One Notebook from Another
# Notebook A - Ingest CSV data, clean it, and save to Delta
student_df = spark.read.csv("/Filestore/student_data.csv", header=True)

# Clean data (remove duplicates and handle missing values)
cleaned_student_df = student_df.dropDuplicates().na.drop()

# Save cleaned data to Delta table
cleaned_student_df.write.format("delta").mode("overwrite").save("/delta/cleaned_student_data")

# Run Notebook B from Notebook A
dbutils.notebook.run("/path/to/notebook_b", 60)

# Notebook B - Perform analysis on the cleaned data from Delta table
cleaned_student_df = spark.read.format("delta").load("/mnt/delta/cleaned_student_data")

# Calculate average score
avg_score_df = cleaned_student_df.groupBy("Class").agg({"Score": "avg"})

# Save analysis results to Delta table
avg_score_df.write.format("delta").mode("overwrite").save("/delta/student_avg_score")

# Task 4: Databricks Ingestion
# Read CSV File from Azure Data Lake:
azure_csv_path =
"abfss://<file_system>@<storage_account>.dfs.core.windows.net/student_info.csv"
student_df = spark.read.csv(azure_csv_path, header=True)

# Read JSON File from Databricks FileStore:
json_path = "dbfs:/FileStore/city_info.json"
city_df = spark.read.json(json_path)

# Read Parquet File from AWS S3:
s3_parquet_path = "s3a://bucket-name/path/to/hospital_info.parquet"

hospital_df = spark.read.parquet(s3_parquet_path)

# Read Delta Table from Databricks-managed Database:
hospital_delta_df = spark.read.format("delta").load("/mnt/delta/hospital_data")

# Write Cleaned Data to Various Formats After Transformations:
# Filtering rows where Class >= 10
transformed_student_df = student_df.filter(student_df.Class >= 10)

# Write to CSV
transformed_student_df.write.csv("/Filestore/output/transformed_student_data.csv", header=True)

# Write to JSON
city_df.write.json("/Filestore/output/transformed_city_data.json")

# Write to Parquet
hospital_df.write.parquet("/Filestore/output/transformed_hospital_data.parquet")

# Write to Delta
hospital_df.write.format("delta").save("/delta/transformed_hospital_data")

# Additional Tasks:
# Optimization Task: Optimize Delta Table
spark.sql("OPTIMIZE '/delta/hospital_data'")

# Z-ordering Task: Apply Z-ordering on the CityName or Class Column
spark.sql("OPTIMIZE '/delta/hospital_data' ZORDER BY (CityName)")

# Vacuum Task: Clean up Old Versions of the Delta Table
spark.sql("VACUUM '/delta/hospital_data' RETAIN 0 HOURS")

# Exercise 1: Creating a Complete ETL Pipeline using Delta Live Tables (DLT)
# Task 1: Create Delta Live Table (DLT) Pipeline
# Task 2: Write DLT in Python
import dlt
from pyspark.sql.functions import col
# Step 1: Create the Raw Transactions Table
@dlt.table
def raw_transactions():
return (
spark.read.format("csv")
.option("header", True)
.option("inferSchema", True)
.load("/path/to/transactions.csv")
)

# Step 2: Create the Transformed Transactions Table
@dlt.table
def transformed_transactions():
return (
dlt.read("raw_transactions")
.withColumn("TotalAmount", col("Quantity") * col("Price"))
)

# Step 3: Write to Delta Table
@dlt.table
def final_transactions():
return dlt.read("transformed_transactions")

# Task 3: Write DLT in SQL
-- Step 1: Create Raw Transactions Table
CREATE OR REPLACE LIVE TABLE raw_transactions AS

SELECT * FROM read_csv('/path/to/transactions.csv');

-- Step 2: Transform Data by Calculating TotalAmount
CREATE OR REPLACE LIVE TABLE transformed_transactions AS
SELECT *, Quantity * Price AS TotalAmount
FROM raw_transactions;

-- Step 3: Store Final Transformed Data into Delta Table
CREATE OR REPLACE LIVE TABLE final_transactions AS
SELECT * FROM transformed_transactions;

# Task 4: Monitor the Pipeline
# Use Databricks' DLT UI to monitor the pipeline and check the status of
each step.
# Exercise 2: Delta Lake Operations - Read, Write, Update, Delete, Merge
# Task 1: Read Data from Delta Lake
# Read Data using PySpark:
delta_table_path = "/path/to/transactions/delta_table"
df = spark.read.format("delta").load(delta_table_path)
# Display first 5 rows of the data
df.show(5)

# Read Data using SQL:
SELECT * FROM delta.`/path/to/transactions/delta_table`
LIMIT 5;

# Task 2: Write Data to Delta Lake (Append New Transactions)
# Create a DataFrame with the new transactions
new_data = [(6, "2024-09-06", "C005", "Keyboard", 4, 100),
(7, "2024-09-07", "C006", "Mouse", 10, 20)]

columns = ["TransactionID", "TransactionDate", "CustomerID", "Product", "Quantity", "Price"]

new_df = spark.createDataFrame(new_data, columns)

# Append the new data to the Delta table
new_df.write.format("delta").mode("append").save(delta_table_path)

# Task 3: Update Data in Delta Lake (Update the Price of 'Laptop')
-- SQL to update the price of 'Laptop' to 1300
UPDATE delta.`/path/to/transactions/delta_table`
SET Price = 1300
WHERE Product = 'Laptop';

-- Verify the update
SELECT * FROM delta.`/path/to/transactions/delta_table`
WHERE Product = 'Laptop';

# Task 4: Delete Data from Delta Lake (Delete Transactions with Quantity < 3)
# Delete rows where Quantity is less than 3
delta_table.delete("Quantity < 3")

# Verify the deletion
delta_table.toDF().show()

-- SQL to delete rows where Quantity is less than 3
DELETE FROM delta.`/path/to/transactions/delta_table`
WHERE Quantity < 3;

-- Verify the deletion
SELECT * FROM delta.`/path/to/transactions/delta_table`;

# Task 5: Merge Data into Delta Lake (Insert and Update Transactions)
# New data for merge operation
merge_data = [(1, "2024-09-01", "C001", "Laptop", 1, 1250),
(8, "2024-09-08", "C007", "Charger", 2, 30)]

merge_df = spark.createDataFrame(merge_data, columns)

# Perform the merge operation
delta_table.alias("target").merge(
merge_df.alias("source"),
"target.TransactionID = source.TransactionID"
).whenMatchedUpdateAll(
).whenNotMatchedInsertAll(
).execute()

# Verify the merge operation
delta_table.toDF().show()

# Exercise 3: Delta Lake - History, Time Travel, and Vacuum
# Task 1: View Delta Table History
# View History using PySpark:
from delta.tables import *

# Load the Delta table
delta_table = DeltaTable.forPath(spark, "/path/to/transactions/delta_table")

# View the history of the Delta table
history_df = delta_table.history()

# Display the last 10 operations
history_df.show(10)

# View History using SQL:
-- SQL to view the history of the Delta table
DESCRIBE HISTORY delta.`/path/to/transactions/delta_table`;

-- View the last 10 operations
SELECT * FROM delta.`/path/to/transactions/delta_table` VERSION AS OF 10;

# Task 2: Perform Time Travel
# Load the Delta table as it was 5 versions ago
df_time_travel = spark.read.format("delta").option("versionAsOf",
5).load("/path/to/transactions/delta_table")

# Show the data as it was 5 versions ago
df_time_travel.show()

# Load the Delta table as it was at a specific timestamp (YYYY-MM-DD HH:MM:SS)
df_time_travel = spark.read.format("delta").option("timestampAsOf", "2024-09-01
14:30:00").load("/path/to/transactions/delta_table")

# Show the data from that timestamp
df_time_travel.show()

-- SQL to retrieve the state of the table 5 versions ago
SELECT * FROM delta.`/path/to/transactions/delta_table` VERSION AS OF 5;

-- SQL to retrieve the state of the table at a specific timestamp
SELECT * FROM delta.`/path/to/transactions/delta_table` TIMESTAMP AS OF '2024-09-01 14:30:00';

# Task 3: Vacuum the Delta Table
-- SQL to vacuum the Delta table with a retention period of 7 days
VACUUM delta.`/path/to/transactions/delta_table` RETAIN 168 HOURS;

-- Verify that the current table state is intact
SELECT * FROM delta.`/path/to/transactions/delta_table`;

# Task 4: Converting Parquet Files to Delta Files
# Load raw transaction CSV data
csv_file_path = "/path/to/raw_transactions.csv"
df_csv = spark.read.option("header", "true").csv(csv_file_path)

# Save as a Parquet table
parquet_table_path = "/path/to/parquet_transactions"
df_csv.write.format("parquet").save(parquet_table_path)

# Convert the Parquet table to a Delta table
parquet_df = spark.read.format("parquet").load(parquet_table_path)
parquet_df.write.format("delta").save("/path/to/delta_transactions")

-- SQL to convert Parquet table to Delta
CONVERT TO DELTA parquet.`/path/to/parquet_transactions`;

# Exercise 4: Implementing Incremental Load Pattern using Delta Lake
# Task 1: Set Up Initial Data
# Load initial transactions data (from 2024-09-01 to 2024-09-03)
initial_data = [
(1, '2024-09-01', 'C001', 'Laptop', 1, 1200),
(2, '2024-09-02', 'C002', 'Tablet', 2, 300),
(3, '2024-09-03', 'C001', 'Headphones', 5, 50)
]

# Define the schema
schema = ['TransactionID', 'TransactionDate', 'CustomerID', 'Product', 'Quantity', 'Price']

# Create DataFrame
initial_df = spark.createDataFrame(initial_data, schema=schema)

# Write the initial data to a Delta table
initial_df.write.format("delta").mode("overwrite").save("/path/to/delta/transactions")

# Task 2: Set Up Incremental Data
# New transactions data (from 2024-09-04 to 2024-09-07)
incremental_data = [
(4, '2024-09-04', 'C003', 'Smartphone', 1, 800),
(5, '2024-09-05', 'C004', 'Smartwatch', 3, 200),
(6, '2024-09-06', 'C005', 'Keyboard', 4, 100),
(7, '2024-09-07', 'C006', 'Mouse', 10, 20)
]

# Create DataFrame for incremental data
incremental_df = spark.createDataFrame(incremental_data, schema=schema)

# Task 3: Implement Incremental Load
# Read the existing Delta table (initial data from 2024-09-01 to 2024-09-03)
existing_df = spark.read.format("delta").load("/path/to/delta/transactions")

# Show the existing data
existing_df.show()

# Append new transactions (from 2024-09-04 to 2024-09-07) to the Delta table
incremental_df.write.format("delta").mode("append").save("/path/to/delta/transactions")

# Verify that the new transactions have been appended without overwriting
updated_df = spark.read.format("delta").load("/path/to/delta/transactions")

updated_df.show()

# Task 4: Monitor Incremental Load
-- SQL to view the version history of the Delta table
DESCRIBE HISTORY delta.`/path/to/delta/transactions`;