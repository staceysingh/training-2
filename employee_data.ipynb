{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/staceysingh/training-2/blob/main/employee_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "063c8a5a-32a4-4f0f-ad2c-f7b1fce4c9c7",
          "showTitle": false,
          "title": ""
        },
        "id": "XSC2IMr0ILgY",
        "outputId": "a809f45b-e169-4ece-8737-6b86f2105de9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {}
        }
      ],
      "source": [
        "# Move the file from Workspace to DBFS\n",
        "dbutils.fs.cp(\"file:/Workspace/Shared/employee_data.csv\", \"dbfs:/FileStore/employee_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1f8649f8-9edc-4785-82c0-46beae0ad754",
          "showTitle": false,
          "title": ""
        },
        "id": "dNUW4DERILgb"
      },
      "outputs": [],
      "source": [
        "# Load the file from DBFS\n",
        "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/FileStore/employee_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3b2bf430-baf4-4585-b1bc-a2377f27709e",
          "showTitle": false,
          "title": ""
        },
        "id": "x2wbxXcQILgc",
        "outputId": "6a258c38-60b3-49c4-d513-335efb7e3ff1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+----------+-----------+------+\n|EmployeeID|         Name|Department|JoiningDate|Salary|\n+----------+-------------+----------+-----------+------+\n|      1001|     John Doe|        HR| 2021-01-15| 55000|\n|      1002|   Jane Smith|        IT| 2020-03-10| 62000|\n|      1003|Emily Johnson|   Finance| 2019-07-01| 70000|\n|      1004|Michael Brown|        HR| 2018-12-22| 54000|\n|      1005| David Wilson|        IT| 2021-06-25| 58000|\n|      1006|  Linda Davis|   Finance| 2020-11-15| 67000|\n|      1007| James Miller|        IT| 2019-08-14| 65000|\n|      1008|Barbara Moore|        HR| 2021-03-29| 53000|\n+----------+-------------+----------+-----------+------+\n\n"
          ]
        }
      ],
      "source": [
        "#Read the data from the csv file\n",
        "df_csv=spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"dbfs:/FileStore/employee_data.csv\")\n",
        "df_csv.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "7735decb-54ac-44b2-9292-58049bd56b77",
          "showTitle": false,
          "title": ""
        },
        "id": "eV8hGkEgILgc",
        "outputId": "ec04b2ad-1910-49a5-e012-b24681f72e02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+----------+-----------+------+\n|EmployeeID|         Name|Department|JoiningDate|Salary|\n+----------+-------------+----------+-----------+------+\n|      1001|     John Doe|        HR| 2021-01-15| 55000|\n|      1002|   Jane Smith|        IT| 2020-03-10| 62000|\n|      1003|Emily Johnson|   Finance| 2019-07-01| 70000|\n|      1004|Michael Brown|        HR| 2018-12-22| 54000|\n|      1005| David Wilson|        IT| 2021-06-25| 58000|\n|      1006|  Linda Davis|   Finance| 2020-11-15| 67000|\n|      1007| James Miller|        IT| 2019-08-14| 65000|\n|      1008|Barbara Moore|        HR| 2021-03-29| 53000|\n+----------+-------------+----------+-----------+------+\n\nroot\n |-- EmployeeID: integer (nullable = true)\n |-- Name: string (nullable = true)\n |-- Department: string (nullable = true)\n |-- JoiningDate: date (nullable = true)\n |-- Salary: integer (nullable = true)\n\n"
          ]
        }
      ],
      "source": [
        "# Assignment 1: Working with CSV Data (employee_data.csv)\n",
        "# 1. Load the CSV data\n",
        "df_employee = spark.read.csv('/FileStore/employee_data.csv', header=True, inferSchema=True)\n",
        "df_employee.show(10)\n",
        "df_employee.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "f8d8add0-e328-42cc-a665-c7ddb7cdb7b7",
          "showTitle": false,
          "title": ""
        },
        "id": "Wz7dRcadILgd",
        "outputId": "8921b78f-8c4f-4802-acba-bba6cd685ef0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+----------+-----------+------+\n|EmployeeID|        Name|Department|JoiningDate|Salary|\n+----------+------------+----------+-----------+------+\n|      1001|    John Doe|        HR| 2021-01-15| 55000|\n|      1002|  Jane Smith|        IT| 2020-03-10| 62000|\n|      1005|David Wilson|        IT| 2021-06-25| 58000|\n|      1006| Linda Davis|   Finance| 2020-11-15| 67000|\n+----------+------------+----------+-----------+------+\n\n"
          ]
        }
      ],
      "source": [
        "# 2. Data Cleaning:\n",
        "# Remove rows where the Salary is less than 55,000.\n",
        "# Filter the employees who joined after the year 2020.\n",
        "df_cleaned = df_employee.filter((df_employee['Salary'] >= 55000) & (df_employee['JoiningDate'] > '2020-01-01'))\n",
        "df_cleaned.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "6270d743-99a4-4e14-9773-b0442fd4f975",
          "showTitle": false,
          "title": ""
        },
        "id": "exTq9zY9ILgd",
        "outputId": "22d5f66b-7208-43d1-ffd8-649efeb95c3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+\n|Department|AvgSalary|\n+----------+---------+\n|        HR|  55000.0|\n|   Finance|  67000.0|\n|        IT|  60000.0|\n+----------+---------+\n\n+----------+-------------+\n|Department|EmployeeCount|\n+----------+-------------+\n|        HR|            1|\n|   Finance|            1|\n|        IT|            2|\n+----------+-------------+\n\n"
          ]
        }
      ],
      "source": [
        "# 3. Data Aggregation:\n",
        "# Find the average salary by Department.\n",
        "df_avg_salary_by_dept = df_cleaned.groupBy('Department').agg({'Salary': 'avg'}).withColumnRenamed('avg(Salary)', 'AvgSalary')\n",
        "df_avg_salary_by_dept.show()\n",
        "\n",
        "# Count the number of employees in each Department.\n",
        "df_count_by_dept = df_cleaned.groupBy('Department').count().withColumnRenamed('count', 'EmployeeCount')\n",
        "df_count_by_dept.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "dbbd0c60-1507-49c2-9be5-542836a23653",
          "showTitle": false,
          "title": ""
        },
        "id": "l69DBHUXILgd"
      },
      "outputs": [],
      "source": [
        "# 4. Write the Data to CSV:\n",
        "# Save the cleaned data (from the previous steps) to a new CSV file.\n",
        "df_cleaned.coalesce(1).write.csv('/dbfs/FileStore/cleaned_employee_data.csv', header=True)\n"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "environmentMetadata": null,
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "employee_data",
      "widgets": {}
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}